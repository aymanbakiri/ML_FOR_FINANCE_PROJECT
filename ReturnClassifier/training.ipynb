{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "\n",
    "from data_preprocessing.merge import df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56a0c9",
   "metadata": {},
   "source": [
    "## CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f1fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    # Price/technical:\n",
    "    'momentum_3m', 'momentum_6m', 'momentum_12m', 'volatility_12m',\n",
    "\n",
    "    # Basic YTD fundamentals (optional—tree can split on scale):\n",
    "    'revty', 'saley', 'capxy', 'oibdpy', 'rdipay', 'xsgay', 'txpdy', 'epsfxy', 'cshfdy', 'xoptepsy',\n",
    "\n",
    "    # Engineered ratios:\n",
    "    'EBIT_margin', 'R&D_intensity', 'SGA_intensity', 'Tax_rate', 'Capex_to_Revenue',\n",
    "\n",
    "    # QoQ growth rates:\n",
    "    'revty_QoQ_growth', 'oibdpy_QoQ_growth', 'rdipay_QoQ_growth', 'xsgay_QoQ_growth',\n",
    "\n",
    "    # Months\n",
    "    'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December',\n",
    "\n",
    "    # Indicators\n",
    "    'EMA', 'Volatility', 'RSI', 'MACD'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57556de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows where engineered features are NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df = df.dropna(subset=feature_columns + ['y']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_columns]\n",
    "y = df['y']\n",
    "\n",
    "# Instead of a fixed 80/20 cutoff, we build an expanding‐window cross‐validation\n",
    "# but we keep a final out‐of‐sample test set (last 20% of months).\n",
    "n_obs = len(df)\n",
    "cutpoint = int(n_obs * 0.8)\n",
    "\n",
    "X_train = X.iloc[:cutpoint]\n",
    "y_train = y.iloc[:cutpoint]\n",
    "\n",
    "X_test  = X.iloc[cutpoint:]\n",
    "y_test  = y.iloc[cutpoint:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67839e7f",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c927ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.rf_classifier import build_rf_pipeline\n",
    "\n",
    "pipe, param_grid = build_rf_pipeline()\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212063f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import report\n",
    "\n",
    "report(X_test, y_train, y_test,  best_model, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86d212",
   "metadata": {},
   "source": [
    "### XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba95a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.xgb_classifier import build_xgb_pipeline\n",
    "\n",
    "pipe, param_grid = build_xgb_pipeline()\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=TimeSeriesSplit(n_splits=5), scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"XGBoost Best params:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35aaaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(X_test, y_train, y_test,  best_model, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6825ea1d",
   "metadata": {},
   "source": [
    "## DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, precision_recall_curve, auc, accuracy_score\n",
    ")\n",
    "\n",
    "# Device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194141e1",
   "metadata": {},
   "source": [
    "### \"EnhancedLSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_sequences, split_data, make_dataloaders, make_loss\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "# Group key\n",
    "group_key = 'PERMNO' if 'PERMNO' in df_scaled.columns else 'CUSIP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ddb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequences with 24-month window\n",
    "WINDOW     = 24\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "X, y = build_sequences(\n",
    "    df            = df_scaled,\n",
    "    feature_columns = feature_columns,\n",
    "    label_column  = 'y',\n",
    "    group_key     = group_key,\n",
    "    window        = WINDOW\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = split_data(X, y, train_frac=0.8, val_frac=0.2)\n",
    "\n",
    "# DataLoaders\n",
    "dl_train, dl_val, dl_test = make_dataloaders(splits, BATCH_SIZE)\n",
    "\n",
    "# Loss\n",
    "_, (X_val, y_val), _ = splits\n",
    "criterion = make_loss(splits[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nn_models import EnhancedLSTM\n",
    "from utils import train, evaluate\n",
    "\n",
    "model = EnhancedLSTM(in_dim=len(feature_columns)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=4)\n",
    "clip_grad = 1.0\n",
    "\n",
    "# Training loop\n",
    "best_auc = 0.0\n",
    "patience, trials = 12, 0\n",
    "epochs= 100\n",
    "\n",
    "train(epochs, model, scheduler, clip_grad, \n",
    "      optimizer, patience, criterion, dl_val, \n",
    "      dl_train=dl_train, device=device, early_stop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f4ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model=model, dl_test=dl_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd3a88",
   "metadata": {},
   "source": [
    "### \"SmallLSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 6\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "X, y = build_sequences(\n",
    "    df            = df_scaled,\n",
    "    feature_columns = feature_columns,\n",
    "    label_column  = 'y',\n",
    "    group_key     = group_key,\n",
    "    window        = WINDOW\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = split_data(X, y, train_frac=0.8, val_frac=0.2)\n",
    "\n",
    "# DataLoaders\n",
    "dl_train, dl_val, dl_test = make_dataloaders(splits, BATCH_SIZE)\n",
    "\n",
    "# Loss\n",
    "_, (X_val, y_val), _ = splits\n",
    "criterion = make_loss(splits[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nn_models import SmallLSTM\n",
    "\n",
    "\n",
    "model = SmallLSTM(len(feature_columns)).to(device)\n",
    "\n",
    "pos_w = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.tensor(pos_w, device=device, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "train(epochs, model, scheduler, clip_grad, \n",
    "      optimizer, patience, criterion, dl_val, \n",
    "      dl_train=dl_train, device=device, early_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model=model, dl_test=dl_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de1a68",
   "metadata": {},
   "source": [
    "### \"LargeLSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd936b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nn_models import LargeLSTM\n",
    "\n",
    "\n",
    "model = LargeLSTM(len(feature_columns)).to(device)\n",
    "\n",
    "pos_w = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.tensor(pos_w, device=device, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "train(epochs, model, scheduler, clip_grad, \n",
    "      optimizer, patience, criterion, dl_val, \n",
    "      dl_train=dl_train, device=device, early_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model=model, dl_test=dl_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6b4f1",
   "metadata": {},
   "source": [
    "### \"StockTransformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7529de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nn_models import StockTransformer\n",
    "\n",
    "\n",
    "model = StockTransformer(len(feature_columns), window=WINDOW).to(device)\n",
    "\n",
    "# Loss & optimizer (with class weighting)\n",
    "pos_w = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.tensor(pos_w, device=device, dtype=torch.float32)\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "\n",
    "best_val_auc = 0.0\n",
    "patience, trials = 5, 0\n",
    "\n",
    "\n",
    "train(epochs, model, scheduler, clip_grad, \n",
    "      optimizer, patience, criterion, dl_val, \n",
    "      dl_train=dl_train, device=device, early_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model=model, dl_test=dl_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265649d3",
   "metadata": {},
   "source": [
    "### \"InceptionModule\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nn_models import InceptionModule, InceptionTime\n",
    "\n",
    "model = InceptionTime(\n",
    "    in_dim=len(feature_columns),\n",
    "    num_blocks=3,\n",
    "    out_channels=32,\n",
    "    kernel_sizes=[3,5,7],\n",
    "    bottleneck_channels=32,\n",
    "    use_residual=True,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "\n",
    "pos_w = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.tensor(pos_w, device=device, dtype=torch.float32)\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "\n",
    "best_val_auc = 0.0\n",
    "patience, trials = 5, 0\n",
    "\n",
    "train(epochs, model, scheduler, clip_grad, \n",
    "      optimizer, patience, criterion, dl_val, \n",
    "      dl_train=dl_train, device=device, early_stop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df86c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model=model, dl_test=dl_test, device=device, inception=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
