{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3438676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96df90f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zy/4vl7h4d95kd9jdq_p6sj2ylw0000gq/T/ipykernel_78592/616781079.py:15: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_comp = pd.read_csv(\n",
      "/var/folders/zy/4vl7h4d95kd9jdq_p6sj2ylw0000gq/T/ipykernel_78592/616781079.py:15: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df_comp = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "# Load Monthly CRSP\n",
    "\n",
    "CRSP_PATH = '../data/monthly_crsp.csv'\n",
    "df_crsp = pd.read_csv(\n",
    "    CRSP_PATH ,\n",
    "    parse_dates=['MthCalDt'],\n",
    "    usecols=['PERMNO','CUSIP','MthCalDt','MthRet']\n",
    ")\n",
    "\n",
    "\n",
    "# Load Compustat Fundamentals\n",
    "\n",
    "COMP_PATH = '../data/CompFirmCharac.csv'\n",
    "\n",
    "df_comp = pd.read_csv(\n",
    "    COMP_PATH,\n",
    "    parse_dates=['datadate'], dayfirst=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c01ee4",
   "metadata": {},
   "source": [
    "### CLEAN CRSP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ecff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1: Keep only rows where MthRet is available and cast to float\n",
    "df_crsp = df_crsp.dropna(subset=['MthRet']).copy()\n",
    "df_crsp['MthRet'] = df_crsp['MthRet'].astype(float)\n",
    "\n",
    "# 2.2: Sort by CUSIP, date so that shift is correct\n",
    "df_crsp['date'] = pd.to_datetime(df_crsp['MthCalDt'].astype(str), format='mixed')\n",
    "df_crsp = df_crsp.sort_values(['CUSIP','date']).reset_index(drop=True)\n",
    "\n",
    "# 2.3: Create next‐month return target (binary)\n",
    "df_crsp['Ret_t1'] = df_crsp.groupby('CUSIP')['MthRet'].shift(-1)\n",
    "# df_crsp['y'] = df_crsp.groupby('CUSIP')['MthRet'].shift(-1)\n",
    "df_crsp['y'] = (df_crsp['Ret_t1'] > 0).astype(int)\n",
    "df_crsp = df_crsp.dropna(subset=['y']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b6b77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            CUSIP   MthCalDt    MthRet       date    Ret_t1       EMA  \\\n",
      "2970518  68391610 1987-03-31 -0.384615 1987-03-31 -0.062500 -4.416586   \n",
      "2970519  68391610 1987-04-30 -0.062500 1987-04-30 -0.066667 -4.140666   \n",
      "1746075  39040610 1987-03-31  0.037486 1987-03-31 -0.039216  0.028843   \n",
      "1746076  39040610 1987-04-30 -0.039216 1987-04-30 -0.071429 -0.009357   \n",
      "1746077  39040610 1987-05-29 -0.071429 1987-05-29  0.052687 -0.075400   \n",
      "...           ...        ...       ...        ...       ...       ...   \n",
      "3917811  88160R10 2024-07-31  0.172781 2024-07-31 -0.077390  0.094132   \n",
      "3917812  88160R10 2024-08-30 -0.077390 2024-08-30  0.221942  0.015727   \n",
      "3917813  88160R10 2024-09-30  0.221942 2024-09-30 -0.045025  0.168567   \n",
      "3917814  88160R10 2024-10-31 -0.045025 2024-10-31  0.381469  0.112118   \n",
      "3917815  88160R10 2024-11-29  0.381469 2024-11-29  0.170008  0.309653   \n",
      "\n",
      "         Volatility        RSI      MACD  April  ...  February  January  July  \\\n",
      "2970518    6.008675  31.218276 -1.072760      0  ...         0        0     0   \n",
      "2970519    6.416235  22.507136 -1.026429      1  ...         0        0     0   \n",
      "1746075    0.058017  67.902789  0.006527      0  ...         0        0     0   \n",
      "1746076    0.057093  56.446075  0.001278      1  ...         0        0     0   \n",
      "1746077    0.065004  46.313294 -0.007746      0  ...         0        0     0   \n",
      "...             ...        ...       ...    ...  ...       ...      ...   ...   \n",
      "3917811    0.146526  54.063213 -0.009722      0  ...         0        0     1   \n",
      "3917812    0.148719  42.222613 -0.005101      0  ...         0        0     0   \n",
      "3917813    0.119113  49.167329  0.010568      0  ...         0        0     0   \n",
      "3917814    0.121613  48.826636  0.017277      0  ...         0        0     0   \n",
      "3917815    0.131800  60.834500  0.032353      0  ...         0        0     0   \n",
      "\n",
      "         June  March  May  November  October  September  y  \n",
      "2970518     0      1    0         0        0          0  0  \n",
      "2970519     0      0    0         0        0          0  0  \n",
      "1746075     0      1    0         0        0          0  0  \n",
      "1746076     0      0    0         0        0          0  0  \n",
      "1746077     0      0    1         0        0          0  1  \n",
      "...       ...    ...  ...       ...      ...        ... ..  \n",
      "3917811     0      0    0         0        0          0  0  \n",
      "3917812     0      0    0         0        0          0  1  \n",
      "3917813     0      0    0         0        0          1  0  \n",
      "3917814     0      0    0         0        1          0  1  \n",
      "3917815     0      0    0         1        0          0  1  \n",
      "\n",
      "[3997219 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "\n",
    "# Add technical indicators and months\n",
    "\n",
    "def compute_close(y_series):\n",
    "    close = (1 + y_series.fillna(0)).cumprod()\n",
    "    close.iloc[0] = 1.0\n",
    "    return close\n",
    "\n",
    "df_crsp = df_crsp.sort_values([\"PERMNO\", \"date\"])\n",
    "df_crsp[\"close\"] = df_crsp.groupby(\"PERMNO\")[\"MthRet\"].apply(compute_close).reset_index(level=0, drop=True)\n",
    "\n",
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    ma_up = up.rolling(window).mean()\n",
    "    ma_down = down.rolling(window).mean()\n",
    "    rs = ma_up / ma_down\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "# Group-wise calculations\n",
    "def add_technical_indicators(group):\n",
    "    group = group.copy()\n",
    "    group['EMA_Close'] = group['close'].ewm(span=14, adjust=False).mean()\n",
    "    group['EMA'] = (group['close'] - group['EMA_Close'])/ group['close']\n",
    "\n",
    "    rolling_std = group['close'].rolling(window=14).std()\n",
    "    group['Volatility'] = rolling_std / group['close']\n",
    "\n",
    "    group['RSI'] = calculate_rsi(group['close'])\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = group['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = group['close'].ewm(span=26, adjust=False).mean()\n",
    "    group['MACD_diff'] = ema12 - ema26\n",
    "    group['MACD_Signal'] = group['MACD_diff'].ewm(span=9, adjust=False).mean()\n",
    "    group['MACD'] = (group['MACD_diff'] - group['MACD_Signal']) / group['close']\n",
    "\n",
    "\n",
    "    return group\n",
    "\n",
    "# Apply technical indicators to each stock (PERMNO)\n",
    "df_crsp = df_crsp.groupby(\"PERMNO\", group_keys=False).apply(add_technical_indicators, include_groups=False)\n",
    "\n",
    "df_crsp.drop(columns=[\"close\", \"EMA_Close\", \"MACD_diff\", \"MACD_Signal\", 'y_forward'], inplace=True, errors='ignore')\n",
    "\n",
    "# Add months\n",
    "df_crsp['month'] = df_crsp['date'].dt.month.map(lambda x: calendar.month_name[x])\n",
    "\n",
    "# Create dummy variables with month names as column names\n",
    "month_dummies = pd.get_dummies(df_crsp['month']).astype(int)\n",
    "\n",
    "# Concatenate dummies with original dataframe\n",
    "df_crsp = pd.concat([df_crsp, month_dummies], axis=1)\n",
    "\n",
    "# drop the intermediate 'month' column \n",
    "df_crsp = df_crsp.drop(columns=['month'])\n",
    "\n",
    "# Get the current columns\n",
    "cols = list(df_crsp.columns)\n",
    "\n",
    "# Move 'y' to the end if it exists\n",
    "if 'y' in cols:\n",
    "    cols.remove('y')\n",
    "    cols.append('y')\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_crsp = df_crsp[cols]\n",
    "\n",
    "df_crsp = df_crsp.dropna()\n",
    "print(df_crsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc8b4c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4: Generate price‐history features (momentum + volatility)\n",
    "#\n",
    "#   - 3M momentum: cumulative return over past 3 months (t-3 → t-1)\n",
    "#   - 6M momentum: cumulative return over past 6 months\n",
    "#   - 12M momentum: cumulative return over past 12 months\n",
    "#   - 12M rolling volatility: std of monthly returns over past 12 months\n",
    "#\n",
    "def compute_momentum_and_vol(df):\n",
    "    df = df.sort_values('date')\n",
    "    # Rolling log(1+return), because cumulative product of (1 + ret) = exp(sum(log(1+ret)))\n",
    "    df['log1p_ret'] = np.log1p(df['MthRet'])\n",
    "    df['log1p_ret_shift1'] = df.groupby('CUSIP')['log1p_ret'].shift(1)\n",
    "    df['cum12_1_log'] = df.groupby('CUSIP')['log1p_ret_shift1'].rolling(window=11).sum().reset_index(0,drop=True)\n",
    "    df['mom_12_1'] = np.expm1(df['cum12_1_log'])\n",
    "    df['cum3m_log'] = df.groupby('CUSIP')['log1p_ret'].rolling(window=3, min_periods=3).sum().reset_index(0,drop=True)\n",
    "    df['cum6m_log'] = df.groupby('CUSIP')['log1p_ret'].rolling(window=6, min_periods=6).sum().reset_index(0,drop=True)\n",
    "    df['cum12m_log'] = df.groupby('CUSIP')['log1p_ret'].rolling(window=12, min_periods=12).sum().reset_index(0,drop=True)\n",
    "    df['momentum_3m'] = np.expm1(df['cum3m_log'])    # exp(sum)-1 => (1+r1)*(1+r2)*(1+r3) - 1\n",
    "    df['momentum_6m'] = np.expm1(df['cum6m_log'])\n",
    "    df['momentum_12m'] = np.expm1(df['cum12m_log'])\n",
    "    df['volatility_12m'] = df.groupby('CUSIP')['MthRet'].rolling(window=12, min_periods=12).std().reset_index(0,drop=True)\n",
    "    # Drop intermediate log columns\n",
    "    return df.drop(columns=['log1p_ret','cum3m_log','cum6m_log','cum12m_log'])\n",
    "\n",
    "df_crsp = compute_momentum_and_vol(df_crsp)\n",
    "\n",
    "# 2.5: Trim CUSIP to 8 characters (for merging) and drop NA\n",
    "df_crsp['cusip'] = df_crsp['CUSIP'].astype(str).str[:8]\n",
    "df_crsp = df_crsp.dropna(subset=['cusip']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a84c12",
   "metadata": {},
   "source": [
    "### CLEAN COMPFIRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3acc27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1: Keep only Industrial & Consolidated\n",
    "df_comp = df_comp[\n",
    "    (df_comp['consol'] == 'C')\n",
    "].copy()\n",
    "\n",
    "# 3.2: Trim & parse keys/dates\n",
    "df_comp['cusip'] = df_comp['cusip'].astype(str).str[:8]\n",
    "df_comp['datadate'] = pd.to_datetime(df_comp['datadate'])\n",
    "df_comp = df_comp.dropna(subset=['cusip','datadate']).copy()\n",
    "\n",
    "# 3.3: Build “effective_date” = datadate + 45 calendar days,\n",
    "#      so that we only use Q data ~45 days after quarter‐end.\n",
    "df_comp['effective_date'] = df_comp['datadate'] + pd.Timedelta(days=45)\n",
    "df_comp = df_comp.set_index('effective_date').sort_index()\n",
    "\n",
    "# 3.4: Select a larger fundamental set (YTD flows + per‐share metrics)\n",
    "fundamental_cols = [\n",
    "    'revty',    # Revenue YTD\n",
    "    'saley',    # Sales YTD\n",
    "    'capxy',    # CapEx YTD\n",
    "    'oibdpy',   # EBITDA YTD\n",
    "    'rdipay',   # R&D expense YTD\n",
    "    'xsgay',    # SG&A expense YTD\n",
    "    'txpdy',    # Tax provision YTD\n",
    "    'epsfxy',   # Diluted EPS ex‐extra YTD\n",
    "    'cshfdy',   # Diluted shares YTD (millions)\n",
    "    'xoptepsy'  # Option expense per share YTD\n",
    "]\n",
    "\n",
    "df_comp_small = df_comp[['cusip'] + fundamental_cols].copy()\n",
    "\n",
    "# 3.5: For each “cusip + quarter,” drop exact duplicates\n",
    "df_comp_small = df_comp_small.reset_index().drop_duplicates(\n",
    "    subset=['cusip','effective_date']\n",
    ").set_index('effective_date').sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031bbf43",
   "metadata": {},
   "source": [
    "### MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3fc5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1: Set df_crsp index to “date”\n",
    "df_crsp = df_crsp.set_index('date').sort_index()\n",
    "\n",
    "# 4.2: Merge (for every month, get the most recent Compustat row ≤ that month’s date)\n",
    "df_merged = pd.merge_asof(\n",
    "    left = df_crsp.reset_index(),\n",
    "    right = df_comp_small.reset_index(),\n",
    "    left_on = 'date',\n",
    "    right_on = 'effective_date',\n",
    "    by = 'cusip',\n",
    "    direction = 'backward',\n",
    "    allow_exact_matches=True\n",
    ").set_index('date')\n",
    "\n",
    "# 4.3: Drop rows where any of our fundamentals are NA, since we can’t compute ratios otherwise\n",
    "df_merged = df_merged.dropna(subset=fundamental_cols + ['y']).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "570dc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged.copy()\n",
    "\n",
    "# 5.1: Engineer simple ratios (safe‐guard divisions by zero)\n",
    "df['EBIT_margin']      = df['oibdpy'] / df['saley'].replace({0: np.nan})\n",
    "df['R&D_intensity']    = df['rdipay'] / df['saley'].replace({0: np.nan})\n",
    "df['SGA_intensity']    = df['xsgay'] / df['saley'].replace({0: np.nan})\n",
    "df['Tax_rate']         = df['txpdy']  / df['oibdpy'].replace({0: np.nan})\n",
    "df['Capex_to_Revenue'] = df['capxy']  / df['revty'].replace({0: np.nan})\n",
    "\n",
    "# 5.2: Compute QoQ growth rates on YTD fundamentals\n",
    "for col in ['revty','oibdpy','rdipay','xsgay']:\n",
    "    df[col + '_QoQ_growth'] = df.groupby('cusip')[col].pct_change()  # (This QY / Last QY) - 1\n",
    "\n",
    "# 5.3: Optionally drop some raw dollar‐amount YTD columns if you want just ratios\n",
    "#      (otherwise let the model pick scale vs. ratio)\n",
    "# df = df.drop(columns=['revty','saley','capxy','oibdpy','rdipay','xsgay','txpdy','epsfxy','cshfdy','xoptepsy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b27bfe5",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e77625e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    # Price/technical:\n",
    "    'momentum_3m', 'momentum_6m', 'momentum_12m', 'volatility_12m',\n",
    "\n",
    "    # Basic YTD fundamentals (optional—tree can split on scale):\n",
    "    'revty', 'saley', 'capxy', 'oibdpy', 'rdipay', 'xsgay', 'txpdy', 'epsfxy', 'cshfdy', 'xoptepsy',\n",
    "\n",
    "    # Engineered ratios:\n",
    "    'EBIT_margin', 'R&D_intensity', 'SGA_intensity', 'Tax_rate', 'Capex_to_Revenue',\n",
    "\n",
    "    # QoQ growth rates:\n",
    "    'revty_QoQ_growth', 'oibdpy_QoQ_growth', 'rdipay_QoQ_growth', 'xsgay_QoQ_growth',\n",
    "\n",
    "    # Months\n",
    "    'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December',\n",
    "\n",
    "    # Indicators\n",
    "    'EMA', 'Volatility', 'RSI', 'MACD'\n",
    "]\n",
    "\n",
    "# Drop any rows where engineered features are NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df = df.dropna(subset=feature_columns + ['y']).copy()\n",
    "\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9980d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of a fixed 80/20 cutoff, build an expanding‐window cross‐validation\n",
    "# but keep a final out‐of‐sample test set (last 20% of months).\n",
    "n_obs = len(df)\n",
    "cutpoint = int(n_obs * 0.8)\n",
    "\n",
    "X_train = X.iloc[:cutpoint]\n",
    "y_train = y.iloc[:cutpoint]\n",
    "\n",
    "X_test  = X.iloc[cutpoint:]\n",
    "y_test  = y.iloc[cutpoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eddb922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters: {'clf__max_depth': 7, 'clf__max_features': 'log2', 'clf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# pipe = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('reg', RandomForestRegressor(random_state=42))\n",
    "# ])\n",
    "\n",
    "# param_grid = {\n",
    "#     'reg__n_estimators': [100, 200],\n",
    "#     'reg__max_depth': [5, 7, 9],\n",
    "#     'reg__max_features': ['sqrt', 'log2']\n",
    "# }\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler',   StandardScaler()),   # scale ratio features so splits are easier\n",
    "    ('clf',      RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [5, 7, 9],\n",
    "    'clf__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bcb69cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.46      0.43       344\n",
      "           1       0.61      0.55      0.58       531\n",
      "\n",
      "    accuracy                           0.52       875\n",
      "   macro avg       0.51      0.51      0.50       875\n",
      "weighted avg       0.53      0.52      0.52       875\n",
      "\n",
      "Test ROC AUC:  0.5200\n",
      "Test PR AUC:   0.6401\n",
      "Confusion Matrix (low values = better balance):\n",
      " [[159 185]\n",
      " [238 293]]\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "Capex_to_Revenue    0.046911\n",
      "Volatility          0.046534\n",
      "momentum_12m        0.046269\n",
      "momentum_3m         0.042559\n",
      "RSI                 0.042087\n",
      "MACD                0.041529\n",
      "momentum_6m         0.038611\n",
      "volatility_12m      0.038029\n",
      "epsfxy              0.037996\n",
      "SGA_intensity       0.036842\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 9.1: Prediction & Classification Report\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 9.2: ROC AUC + Precision‐Recall AUC\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "print(f\"Test ROC AUC:  {roc_auc:.4f}\")\n",
    "print(f\"Test PR AUC:   {pr_auc:.4f}\")\n",
    "\n",
    "# 9.3: Display a confusion matrix if you like\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix (low values = better balance):\\n\", cm)\n",
    "\n",
    "# ───────────\n",
    "# 10. FEATURE IMPORTANCE AND NEXT STEPS\n",
    "# ───────────\n",
    "\n",
    "importances = best_model.named_steps['clf'].feature_importances_\n",
    "feat_imp = pd.Series(importances, index=feature_columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 Feature Importances:\")\n",
    "print(feat_imp.head(10))\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# print(\"\\nRegression Metrics on Test Set:\")\n",
    "# print(f\"MAE:  {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "# print(f\"MSE:  {mean_squared_error(y_test, y_pred):.4f}\")\n",
    "# print(f\"R²:   {r2_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a489edc",
   "metadata": {},
   "source": [
    "TRAINING 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "654d833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    # Price/technical:\n",
    "    'momentum_3m', 'momentum_6m', 'mom_12_1', 'volatility_12m',\n",
    "\n",
    "    # Basic YTD fundamentals (optional—tree can split on scale):\n",
    "    'revty', 'saley', 'capxy', 'oibdpy', 'rdipay', 'xsgay', 'txpdy', 'epsfxy', 'cshfdy', 'xoptepsy',\n",
    "\n",
    "    # Engineered ratios:\n",
    "    'EBIT_margin', 'R&D_intensity', 'SGA_intensity', 'Tax_rate', 'Capex_to_Revenue',\n",
    "\n",
    "    # QoQ growth rates:\n",
    "    'revty_QoQ_growth', 'oibdpy_QoQ_growth', 'rdipay_QoQ_growth', 'xsgay_QoQ_growth',\n",
    "\n",
    "    # Months\n",
    "    'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December',\n",
    "\n",
    "    # Indicators\n",
    "    'EMA', 'Volatility', 'RSI', 'MACD'\n",
    "]\n",
    "\n",
    "# Drop any rows where engineered features are NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df = df.dropna(subset=feature_columns + ['y']).copy()\n",
    "\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8008f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of a fixed 80/20 cutoff, build an expanding‐window cross‐validation\n",
    "# but keep a final out‐of‐sample test set (last 20% of months).\n",
    "n_obs = len(df)\n",
    "cutpoint = int(n_obs * 0.8)\n",
    "\n",
    "X_train = X.iloc[:cutpoint]\n",
    "y_train = y.iloc[:cutpoint]\n",
    "\n",
    "X_test  = X.iloc[cutpoint:]\n",
    "y_test  = y.iloc[cutpoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e02defc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Best params: {'clf__learning_rate': 0.01, 'clf__max_depth': 3, 'clf__n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ayman/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [21:15:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler',   StandardScaler()),  \n",
    "    ('clf',      XGBClassifier(\n",
    "         objective='binary:logistic',\n",
    "         eval_metric='auc',\n",
    "         use_label_encoder=False,\n",
    "         n_estimators=200,\n",
    "         max_depth=5,\n",
    "         learning_rate=0.05,\n",
    "         random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [3, 5, 7],\n",
    "    'clf__learning_rate': [0.01, 0.05]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=TimeSeriesSplit(n_splits=5), scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"XGBoost Best params:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4326fd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.19      0.27       344\n",
      "           1       0.61      0.83      0.71       531\n",
      "\n",
      "    accuracy                           0.58       875\n",
      "   macro avg       0.52      0.51      0.49       875\n",
      "weighted avg       0.54      0.58      0.53       875\n",
      "\n",
      "Test ROC AUC:  0.5138\n",
      "Test PR AUC:   0.6361\n",
      "Confusion Matrix (low values = better balance):\n",
      " [[ 66 278]\n",
      " [ 88 443]]\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "xoptepsy             0.069767\n",
      "May                  0.067024\n",
      "October              0.051338\n",
      "April                0.049300\n",
      "June                 0.048363\n",
      "momentum_6m          0.047852\n",
      "MACD                 0.044416\n",
      "January              0.042819\n",
      "oibdpy_QoQ_growth    0.042389\n",
      "March                0.042241\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# 9.1: Prediction & Classification Report\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 9.2: ROC AUC + Precision‐Recall AUC\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "print(f\"Test ROC AUC:  {roc_auc:.4f}\")\n",
    "print(f\"Test PR AUC:   {pr_auc:.4f}\")\n",
    "\n",
    "# 9.3: Display a confusion matrix if you like\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix (low values = better balance):\\n\", cm)\n",
    "\n",
    "# ───────────\n",
    "# 10. FEATURE IMPORTANCE AND NEXT STEPS\n",
    "# ───────────\n",
    "\n",
    "importances = best_model.named_steps['clf'].feature_importances_\n",
    "feat_imp = pd.Series(importances, index=feature_columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 Feature Importances:\")\n",
    "print(feat_imp.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe0c207",
   "metadata": {},
   "source": [
    "DEEP LEARNING PART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53f3d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss 0.6839 | Val AUC 0.4226\n",
      "Epoch 02 | Loss 0.6642 | Val AUC 0.4065\n",
      "Epoch 03 | Loss 0.6548 | Val AUC 0.4071\n",
      "Epoch 04 | Loss 0.6459 | Val AUC 0.4131\n",
      "Epoch 05 | Loss 0.6326 | Val AUC 0.4214\n",
      "Epoch 06 | Loss 0.6315 | Val AUC 0.4208\n",
      "Epoch 07 | Loss 0.6214 | Val AUC 0.4202\n",
      "Epoch 08 | Loss 0.6313 | Val AUC 0.4238\n",
      "Epoch 09 | Loss 0.6321 | Val AUC 0.4220\n",
      "Epoch 10 | Loss 0.6258 | Val AUC 0.4250\n",
      "Epoch 11 | Loss 0.6130 | Val AUC 0.4268\n",
      "Epoch 12 | Loss 0.6126 | Val AUC 0.4286\n",
      "Epoch 13 | Loss 0.6111 | Val AUC 0.4292\n",
      "Epoch 14 | Loss 0.6230 | Val AUC 0.4310\n",
      "Epoch 15 | Loss 0.6214 | Val AUC 0.4298\n",
      "Epoch 16 | Loss 0.6225 | Val AUC 0.4310\n",
      "Epoch 17 | Loss 0.6152 | Val AUC 0.4286\n",
      "Epoch 18 | Loss 0.6114 | Val AUC 0.4286\n",
      "Epoch 19 | Loss 0.6050 | Val AUC 0.4268\n",
      "Epoch 20 | Loss 0.6235 | Val AUC 0.4268\n",
      "Epoch 21 | Loss 0.6089 | Val AUC 0.4268\n",
      "Epoch 22 | Loss 0.6029 | Val AUC 0.4268\n",
      "Epoch 23 | Loss 0.6147 | Val AUC 0.4268\n",
      "Epoch 24 | Loss 0.6182 | Val AUC 0.4256\n",
      "Epoch 25 | Loss 0.6091 | Val AUC 0.4256\n",
      "Epoch 26 | Loss 0.6170 | Val AUC 0.4256\n",
      "Early stopping.\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.3261    0.4545    0.3797        33\n",
      "         1.0     0.6842    0.5571    0.6142        70\n",
      "\n",
      "    accuracy                         0.5243       103\n",
      "   macro avg     0.5051    0.5058    0.4970       103\n",
      "weighted avg     0.5695    0.5243    0.5391       103\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15 18]\n",
      " [31 39]]\n",
      "\n",
      "Test ROC AUC:  0.4931\n",
      "Test PR AUC:   0.7093\n",
      "Test Accuracy: 0.5243\n"
     ]
    }
   ],
   "source": [
    "# ───────────\n",
    "# 11. Enhanced LSTM (PyTorch)\n",
    "# ───────────\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, precision_recall_curve, auc, accuracy_score\n",
    ")\n",
    "\n",
    "# 11.1: Scale features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "# 11.2: Group key\n",
    "group_key = 'PERMNO' if 'PERMNO' in df_scaled.columns else 'CUSIP'\n",
    "\n",
    "# 11.3: Build sequences with 24-month window\n",
    "WINDOW = 24\n",
    "Xs, ys = [], []\n",
    "for key_val, grp in df_scaled.groupby(group_key):\n",
    "    grp = grp.sort_index()\n",
    "    arr = grp[feature_columns].values\n",
    "    labels = grp['y'].values\n",
    "    for i in range(WINDOW, len(arr)):\n",
    "        Xs.append(arr[i-WINDOW:i])\n",
    "        ys.append(labels[i])\n",
    "\n",
    "X = np.stack(Xs).astype(np.float32)\n",
    "y = np.array(ys).astype(np.float32)\n",
    "\n",
    "# 11.4: Chronological split\n",
    "n = len(y)\n",
    "cut = int(n * 0.8)\n",
    "X_train_all, X_test = X[:cut], X[cut:]\n",
    "y_train_all, y_test = y[:cut], y[cut:]\n",
    "val_cut = int(len(X_train_all) * 0.8)\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "# 11.5: DataLoaders\n",
    "batch_size = 128\n",
    "to_tensor = lambda a: torch.from_numpy(a)\n",
    "train_ds = TensorDataset(to_tensor(X_train), to_tensor(y_train))\n",
    "val_ds   = TensorDataset(to_tensor(X_val),   to_tensor(y_val))\n",
    "test_ds  = TensorDataset(to_tensor(X_test),  to_tensor(y_test))\n",
    "dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "dl_val   = DataLoader(val_ds,   batch_size=batch_size)\n",
    "dl_test  = DataLoader(test_ds,  batch_size=batch_size)\n",
    "\n",
    "# 11.6: Class weighting & label smoothing\n",
    "pos_weight = torch.tensor((y_train == 0).sum() / (y_train == 1).sum())\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# 11.7: Enhanced LSTM model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class EnhancedLSTM(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=256, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=in_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hid_dim*2)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hid_dim*2, hid_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hid_dim//2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]          # (B, hid_dim*2)\n",
    "        normed = self.norm(last)\n",
    "        return self.head(normed).squeeze(1)\n",
    "\n",
    "model = EnhancedLSTM(in_dim=len(feature_columns)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=4)\n",
    "clip_grad = 1.0\n",
    "\n",
    "# 11.8: Training loop\n",
    "best_auc = 0.0\n",
    "patience, trials = 12, 0\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for xb, yb in dl_train:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl_val:\n",
    "            xb = xb.to(device)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            preds.extend(probs)\n",
    "            trues.extend(yb.numpy())\n",
    "    auc_val = roc_auc_score(trues, preds)\n",
    "    scheduler.step(auc_val)\n",
    "    print(f\"Epoch {epoch:02d} | Loss {np.mean(losses):.4f} | Val AUC {auc_val:.4f}\")\n",
    "    if auc_val > best_auc:\n",
    "        best_auc, trials = auc_val, 0\n",
    "        torch.save(model.state_dict(), 'best_lstm.pt')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# 11.9: Test evaluation + full report\n",
    "model.load_state_dict(torch.load('best_lstm.pt'))\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in dl_test:\n",
    "        xb = xb.to(device)\n",
    "        all_preds.extend(torch.sigmoid(model(xb)).cpu().numpy())\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "# Binarize at 0.5\n",
    "yhat = np.array(all_preds) > 0.5\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(all_labels, yhat, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, yhat))\n",
    "\n",
    "roc = roc_auc_score(all_labels, all_preds)\n",
    "precision, recall, _ = precision_recall_curve(all_labels, all_preds)\n",
    "pr = auc(recall, precision)\n",
    "acc = accuracy_score(all_labels, yhat)\n",
    "\n",
    "print(f\"\\nTest ROC AUC:  {roc:.4f}\")\n",
    "print(f\"Test PR AUC:   {pr:.4f}\")\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5614223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.5936 | Val AUC: 0.4991\n",
      "Epoch 02 | Train Loss: 0.5894 | Val AUC: 0.5505\n",
      "Epoch 03 | Train Loss: 0.5867 | Val AUC: 0.5399\n",
      "Epoch 04 | Train Loss: 0.5827 | Val AUC: 0.5468\n",
      "Epoch 05 | Train Loss: 0.5786 | Val AUC: 0.5569\n",
      "Epoch 06 | Train Loss: 0.5749 | Val AUC: 0.5527\n",
      "Epoch 07 | Train Loss: 0.5690 | Val AUC: 0.5544\n",
      "Epoch 08 | Train Loss: 0.5611 | Val AUC: 0.5608\n",
      "Epoch 09 | Train Loss: 0.5533 | Val AUC: 0.5710\n",
      "Epoch 10 | Train Loss: 0.5424 | Val AUC: 0.5704\n",
      "Epoch 11 | Train Loss: 0.5309 | Val AUC: 0.5644\n",
      "Epoch 12 | Train Loss: 0.5195 | Val AUC: 0.5739\n",
      "Epoch 13 | Train Loss: 0.5091 | Val AUC: 0.5845\n",
      "Epoch 14 | Train Loss: 0.5000 | Val AUC: 0.5791\n",
      "Epoch 15 | Train Loss: 0.4846 | Val AUC: 0.5761\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5271    0.5120    0.5194       209\n",
      "         1.0     0.6151    0.6293    0.6221       259\n",
      "\n",
      "    accuracy                         0.5769       468\n",
      "   macro avg     0.5711    0.5707    0.5708       468\n",
      "weighted avg     0.5758    0.5769    0.5763       468\n",
      "\n",
      "Confusion Matrix (low values = better balance):\n",
      "[[107 102]\n",
      " [ 96 163]]\n",
      "\n",
      "Test ROC AUC:  0.5859\n",
      "Test PR AUC:   0.6325\n",
      "Test Accuracy: 0.5769\n"
     ]
    }
   ],
   "source": [
    "# ───────────\n",
    "# 11. LSTM on M1 (MPS) — lightweight version\n",
    "# ───────────\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, precision_recall_curve, auc, accuracy_score\n",
    ")\n",
    "\n",
    "# 1) Scale features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "# 2) Choose key & build 6-month sequences\n",
    "group_key = 'PERMNO' if 'PERMNO' in df_scaled.columns else 'CUSIP'\n",
    "WINDOW = 6\n",
    "\n",
    "Xs, ys = [], []\n",
    "for key_val, grp in df_scaled.groupby(group_key):\n",
    "    arr = grp.sort_index()[feature_columns].values\n",
    "    lbl = grp['y'].values\n",
    "    for i in range(WINDOW, len(arr)):\n",
    "        Xs.append(arr[i-WINDOW:i])\n",
    "        ys.append(lbl[i])\n",
    "\n",
    "X = np.stack(Xs).astype(np.float32)  # (n_samples, 6, n_features)\n",
    "y = np.array(ys).astype(np.float32)\n",
    "\n",
    "# 3) Chronological split 80/20 train/test, then 80/20 train/val\n",
    "n = len(y)\n",
    "cp = int(n * 0.8)\n",
    "X_train_all, X_test = X[:cp], X[cp:]\n",
    "y_train_all, y_test = y[:cp], y[cp:]\n",
    "val_cut = int(len(X_train_all) * 0.8)\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "# 4) DataLoaders (batch_size=32)\n",
    "bs = 32\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val))\n",
    "test_ds  = TensorDataset(torch.from_numpy(X_test),  torch.from_numpy(y_test))\n",
    "dl_train = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "dl_val   = DataLoader(val_ds,   batch_size=bs)\n",
    "dl_test  = DataLoader(test_ds,  batch_size=bs)\n",
    "\n",
    "# 5) Device: MPS if available, else CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 6) Small LSTM model\n",
    "class SmallLSTM(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=32):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, hid_dim, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hid_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :]).squeeze(1)\n",
    "\n",
    "model = SmallLSTM(len(feature_columns)).to(device)\n",
    "\n",
    "# 7) Loss + optimizer (class-weighted)\n",
    "pos_w = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.tensor(pos_w, device=device, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 8) Train 15 epochs\n",
    "for epoch in range(1, 16):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in dl_train:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / len(dl_train.dataset)\n",
    "\n",
    "    # Validation AUC\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl_val:\n",
    "            xb = xb.to(device)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            preds.extend(probs)\n",
    "            trues.extend(yb.numpy())\n",
    "    val_auc = roc_auc_score(trues, preds)\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {avg_loss:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "# 9) Final evaluation\n",
    "model.eval()\n",
    "test_preds, test_trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in dl_test:\n",
    "        xb = xb.to(device)\n",
    "        test_preds.extend(torch.sigmoid(model(xb)).cpu().numpy())\n",
    "        test_trues.extend(yb.numpy())\n",
    "\n",
    "y_hat = (np.array(test_preds) > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(test_trues, y_hat, digits=4))\n",
    "print(\"Confusion Matrix (low values = better balance):\")\n",
    "print(confusion_matrix(test_trues, y_hat))\n",
    "\n",
    "roc = roc_auc_score(test_trues, test_preds)\n",
    "precision, recall, _ = precision_recall_curve(test_trues, test_preds)\n",
    "pr_auc = auc(recall, precision)\n",
    "acc = accuracy_score(test_trues, y_hat)\n",
    "\n",
    "print(f\"\\nTest ROC AUC:  {roc:.4f}\")\n",
    "print(f\"Test PR AUC:   {pr_auc:.4f}\")\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f18ab20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.5921 | Val AUC: 0.5333\n",
      "Epoch 02 | Train Loss: 0.5862 | Val AUC: 0.5131\n",
      "Epoch 03 | Train Loss: 0.5786 | Val AUC: 0.5611\n",
      "Epoch 04 | Train Loss: 0.5694 | Val AUC: 0.5676\n",
      "Epoch 05 | Train Loss: 0.5599 | Val AUC: 0.6160\n",
      "Epoch 06 | Train Loss: 0.5518 | Val AUC: 0.5645\n",
      "Epoch 07 | Train Loss: 0.5297 | Val AUC: 0.6170\n",
      "Epoch 08 | Train Loss: 0.5077 | Val AUC: 0.5730\n",
      "Epoch 09 | Train Loss: 0.4834 | Val AUC: 0.5968\n",
      "Epoch 10 | Train Loss: 0.4509 | Val AUC: 0.5877\n",
      "Epoch 11 | Train Loss: 0.4129 | Val AUC: 0.5959\n",
      "Epoch 12 | Train Loss: 0.3653 | Val AUC: 0.5782\n",
      "Epoch 13 | Train Loss: 0.3355 | Val AUC: 0.5860\n",
      "Epoch 14 | Train Loss: 0.2971 | Val AUC: 0.5452\n",
      "Epoch 15 | Train Loss: 0.2622 | Val AUC: 0.5521\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5200    0.4976    0.5086       209\n",
      "         1.0     0.6082    0.6293    0.6186       259\n",
      "\n",
      "    accuracy                         0.5705       468\n",
      "   macro avg     0.5641    0.5635    0.5636       468\n",
      "weighted avg     0.5688    0.5705    0.5695       468\n",
      "\n",
      "Confusion Matrix (low values = better balance):\n",
      "[[104 105]\n",
      " [ 96 163]]\n",
      "\n",
      "Test ROC AUC:  0.5807\n",
      "Test PR AUC:   0.6213\n",
      "Test Accuracy: 0.5705\n"
     ]
    }
   ],
   "source": [
    "# 11. LSTM on M1 (MPS) — large version\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, precision_recall_curve, auc, accuracy_score\n",
    ")\n",
    "\n",
    "# 1) Scale features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "# 2) Build 6-month sequences\n",
    "group_key = 'PERMNO' if 'PERMNO' in df_scaled.columns else 'CUSIP'\n",
    "WINDOW = 6\n",
    "\n",
    "Xs, ys = [], []\n",
    "for _, grp in df_scaled.groupby(group_key):\n",
    "    arr = grp.sort_index()[feature_columns].values\n",
    "    lbl = grp['y'].values\n",
    "    for i in range(WINDOW, len(arr)):\n",
    "        Xs.append(arr[i-WINDOW:i])\n",
    "        ys.append(lbl[i])\n",
    "\n",
    "X = np.stack(Xs).astype(np.float32)\n",
    "y = np.array(ys).astype(np.float32)\n",
    "\n",
    "# 3) Chronological split 80/20 train/test, then 80/20 train/val\n",
    "n = len(y)\n",
    "cp = int(n * 0.8)\n",
    "X_train_all, X_test = X[:cp], X[cp:]\n",
    "y_train_all, y_test = y[:cp], y[cp:]\n",
    "val_cut = int(len(X_train_all) * 0.8)\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "# 4) DataLoaders (batch_size=32)\n",
    "bs = 32\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val))\n",
    "test_ds  = TensorDataset(torch.from_numpy(X_test),  torch.from_numpy(y_test))\n",
    "dl_train = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "dl_val   = DataLoader(val_ds,   batch_size=bs)\n",
    "dl_test  = DataLoader(test_ds,  batch_size=bs)\n",
    "\n",
    "# 5) Device: MPS if available, else CPU\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# 6) Large LSTM model\n",
    "class LargeLSTM(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=128, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=in_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hid_dim*2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.fc(last).squeeze(1)\n",
    "\n",
    "model = LargeLSTM(len(feature_columns)).to(device)\n",
    "\n",
    "# 7) Loss + optimizer (class-weighted)\n",
    "pos_w = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.tensor(pos_w, device=device, dtype=torch.float32)\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 8) Train 15 epochs\n",
    "for epoch in range(1, 16):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in dl_train:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / len(dl_train.dataset)\n",
    "\n",
    "    # Validation AUC\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl_val:\n",
    "            xb = xb.to(device)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            preds.extend(probs)\n",
    "            trues.extend(yb.numpy())\n",
    "    val_auc = roc_auc_score(trues, preds)\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {avg_loss:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "# 9) Final evaluation\n",
    "model.eval()\n",
    "test_preds, test_trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in dl_test:\n",
    "        xb = xb.to(device)\n",
    "        test_preds.extend(torch.sigmoid(model(xb)).cpu().numpy())\n",
    "        test_trues.extend(yb.numpy())\n",
    "\n",
    "y_hat = (np.array(test_preds) > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(test_trues, y_hat, digits=4))\n",
    "print(\"Confusion Matrix (low values = better balance):\")\n",
    "print(confusion_matrix(test_trues, y_hat))\n",
    "\n",
    "roc = roc_auc_score(test_trues, test_preds)\n",
    "precision, recall, _ = precision_recall_curve(test_trues, test_preds)\n",
    "pr_auc = auc(recall, precision)\n",
    "acc = accuracy_score(test_trues, y_hat)\n",
    "\n",
    "print(f\"\\nTest ROC AUC:  {roc:.4f}\")\n",
    "print(f\"Test PR AUC:   {pr_auc:.4f}\")\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98bcbdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.5960 | Val AUC: 0.5742\n",
      "Epoch 02 | Train Loss: 0.5952 | Val AUC: 0.5517\n",
      "Epoch 03 | Train Loss: 0.5922 | Val AUC: 0.5405\n",
      "Epoch 04 | Train Loss: 0.5898 | Val AUC: 0.5289\n",
      "Epoch 05 | Train Loss: 0.5879 | Val AUC: 0.5586\n",
      "Epoch 06 | Train Loss: 0.5839 | Val AUC: 0.5559\n",
      "Early stopping at epoch 6\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5257    0.4402    0.4792       209\n",
      "         1.0     0.6007    0.6795    0.6377       259\n",
      "\n",
      "    accuracy                         0.5726       468\n",
      "   macro avg     0.5632    0.5599    0.5584       468\n",
      "weighted avg     0.5672    0.5726    0.5669       468\n",
      "\n",
      "Confusion Matrix (low values = better balance):\n",
      "[[ 92 117]\n",
      " [ 83 176]]\n",
      "\n",
      "Test ROC AUC:  0.6066\n",
      "Test PR AUC:   0.6635\n",
      "Test Accuracy: 0.5726\n"
     ]
    }
   ],
   "source": [
    "# ───────────\n",
    "# 11. Transformer on M1 (MPS)\n",
    "# ───────────\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, precision_recall_curve, auc, accuracy_score\n",
    ")\n",
    "\n",
    "# 1) Scale features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "# 2) Build 6-month sequences\n",
    "group_key = 'PERMNO' if 'PERMNO' in df_scaled.columns else 'CUSIP'\n",
    "WINDOW = 6\n",
    "Xs, ys = [], []\n",
    "for _, grp in df_scaled.groupby(group_key):\n",
    "    arr = grp.sort_index()[feature_columns].values\n",
    "    lbl = grp['y'].values\n",
    "    for i in range(WINDOW, len(arr)):\n",
    "        Xs.append(arr[i-WINDOW:i])\n",
    "        ys.append(lbl[i])\n",
    "X = np.stack(Xs).astype(np.float32)  # (n_samples, WINDOW, n_features)\n",
    "y = np.array(ys).astype(np.float32)\n",
    "\n",
    "# 3) Chronological split 80/20 train/test, then 80/20 train/val\n",
    "n = len(y)\n",
    "cp = int(n * 0.8)\n",
    "X_train_all, X_test = X[:cp], X[cp:]\n",
    "y_train_all, y_test = y[:cp], y[cp:]\n",
    "val_cut = int(len(X_train_all) * 0.8)\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "# 4) DataLoaders\n",
    "bs = 32\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val))\n",
    "test_ds  = TensorDataset(torch.from_numpy(X_test),  torch.from_numpy(y_test))\n",
    "dl_train = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "dl_val   = DataLoader(val_ds,   batch_size=bs)\n",
    "dl_test  = DataLoader(test_ds,  batch_size=bs)\n",
    "\n",
    "# 5) Device selection (MPS or CPU)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# 6) Transformer-based classifier\n",
    "class StockTransformer(nn.Module):\n",
    "    def __init__(self, in_dim, d_model=128, nhead=4, num_layers=2, dim_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(in_dim, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, WINDOW, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, in_dim)\n",
    "        x = self.input_proj(x) + self.pos_embed[:, :x.size(1), :]\n",
    "        out = self.transformer(x)            # (batch, seq_len, d_model)\n",
    "        return self.classifier(out[:, -1, :]).squeeze(1)\n",
    "\n",
    "model = StockTransformer(len(feature_columns)).to(device)\n",
    "\n",
    "# 7) Loss & optimizer (with class weighting)\n",
    "pos_w = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.tensor(pos_w, device=device, dtype=torch.float32)\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "\n",
    "# 8) Train with early stopping on Val AUC\n",
    "best_val_auc = 0.0\n",
    "patience, trials = 5, 0\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in dl_train:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / len(dl_train.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl_val:\n",
    "            xb = xb.to(device)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            preds.extend(probs)\n",
    "            trues.extend(yb.numpy())\n",
    "    val_auc = roc_auc_score(trues, preds)\n",
    "    scheduler.step(val_auc)\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {avg_loss:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc, trials = val_auc, 0\n",
    "        torch.save(model.state_dict(), 'best_transformer.pth')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# 9) Test set evaluation\n",
    "model.load_state_dict(torch.load('best_transformer.pth'))\n",
    "model.eval()\n",
    "test_preds, test_trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in dl_test:\n",
    "        xb = xb.to(device)\n",
    "        test_preds.extend(torch.sigmoid(model(xb)).cpu().numpy())\n",
    "        test_trues.extend(yb.numpy())\n",
    "\n",
    "y_hat = (np.array(test_preds) > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(test_trues, y_hat, digits=4))\n",
    "print(\"Confusion Matrix (low values = better balance):\")\n",
    "print(confusion_matrix(test_trues, y_hat))\n",
    "\n",
    "roc = roc_auc_score(test_trues, test_preds)\n",
    "prec, rec, _ = precision_recall_curve(test_trues, test_preds)\n",
    "print(f\"\\nTest ROC AUC:  {roc:.4f}\")\n",
    "print(f\"Test PR AUC:   {auc(rec, prec):.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(test_trues, y_hat):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c789524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.6048 | Val AUC: 0.5436\n",
      "Epoch 02 | Train Loss: 0.5881 | Val AUC: 0.5549\n",
      "Epoch 03 | Train Loss: 0.5838 | Val AUC: 0.5428\n",
      "Epoch 04 | Train Loss: 0.5742 | Val AUC: 0.5495\n",
      "Epoch 05 | Train Loss: 0.5639 | Val AUC: 0.5139\n",
      "Epoch 06 | Train Loss: 0.5574 | Val AUC: 0.5351\n",
      "Epoch 07 | Train Loss: 0.5401 | Val AUC: 0.5579\n",
      "Epoch 08 | Train Loss: 0.5344 | Val AUC: 0.5609\n",
      "Epoch 09 | Train Loss: 0.5138 | Val AUC: 0.5491\n",
      "Epoch 10 | Train Loss: 0.5203 | Val AUC: 0.5471\n",
      "Epoch 11 | Train Loss: 0.5092 | Val AUC: 0.5371\n",
      "Epoch 12 | Train Loss: 0.5030 | Val AUC: 0.5414\n",
      "Epoch 13 | Train Loss: 0.4758 | Val AUC: 0.5353\n",
      "Early stopping at epoch 13\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5568    0.4689    0.5091       209\n",
      "         1.0     0.6199    0.6988    0.6570       259\n",
      "\n",
      "    accuracy                         0.5962       468\n",
      "   macro avg     0.5883    0.5839    0.5830       468\n",
      "weighted avg     0.5917    0.5962    0.5909       468\n",
      "\n",
      "Confusion Matrix (low values = better balance):\n",
      "[[ 98 111]\n",
      " [ 78 181]]\n",
      "\n",
      "Test ROC AUC:  0.6346\n",
      "Test PR AUC:   0.6853\n",
      "Test Accuracy: 0.5962\n"
     ]
    }
   ],
   "source": [
    "# ───────────\n",
    "# 11. InceptionTime on M1 (MPS)\n",
    "# ───────────\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, precision_recall_curve, auc, accuracy_score\n",
    ")\n",
    "\n",
    "# 1) Scale features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "# 2) Build 6-month sequences\n",
    "group_key = 'PERMNO' if 'PERMNO' in df_scaled.columns else 'CUSIP'\n",
    "WINDOW = 6\n",
    "Xs, ys = [], []\n",
    "for _, grp in df_scaled.groupby(group_key):\n",
    "    arr = grp.sort_index()[feature_columns].values\n",
    "    lbl = grp['y'].values\n",
    "    for i in range(WINDOW, len(arr)):\n",
    "        Xs.append(arr[i-WINDOW:i])\n",
    "        ys.append(lbl[i])\n",
    "X = np.stack(Xs).astype(np.float32)   # (n_samples, WINDOW, n_features)\n",
    "y = np.array(ys).astype(np.float32)\n",
    "\n",
    "# 3) Chronological split 80/20 train/test, then 80/20 train/val\n",
    "n = len(y)\n",
    "cp = int(n * 0.8)\n",
    "X_train_all, X_test = X[:cp], X[cp:]\n",
    "y_train_all, y_test = y[:cp], y[cp:]\n",
    "val_cut = int(len(X_train_all) * 0.8)\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "# 4) DataLoaders\n",
    "bs = 32\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val))\n",
    "test_ds  = TensorDataset(torch.from_numpy(X_test),  torch.from_numpy(y_test))\n",
    "dl_train = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "dl_val   = DataLoader(val_ds,   batch_size=bs)\n",
    "dl_test  = DataLoader(test_ds,  batch_size=bs)\n",
    "\n",
    "# 5) Device\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# 6) InceptionTime Modules\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=[10,20,40], \n",
    "                 bottleneck_channels=32, use_residual=False, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.use_residual = use_residual\n",
    "        # 1x1 Bottleneck\n",
    "        self.bottleneck = nn.Conv1d(in_channels, bottleneck_channels, kernel_size=1) \\\n",
    "                          if in_channels > 1 else nn.Identity()\n",
    "        # Convolutions with different kernel sizes\n",
    "        self.conv_branches = nn.ModuleList([\n",
    "            nn.Conv1d(bottleneck_channels, out_channels, kernel_size=k, padding=k//2)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        # MaxPool branch\n",
    "        self.maxpool_branch = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        )\n",
    "        self.batchnorm = nn.BatchNorm1d(out_channels * (len(kernel_sizes) + 1))\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if self.use_residual:\n",
    "            self.residual = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels * (len(kernel_sizes) + 1), kernel_size=1),\n",
    "                nn.BatchNorm1d(out_channels * (len(kernel_sizes) + 1))\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features) -> for conv: (batch, features, seq_len)\n",
    "        x_in = x.transpose(1, 2)\n",
    "        if hasattr(self, 'bottleneck') and not isinstance(self.bottleneck, nn.Identity):\n",
    "            x_b = self.bottleneck(x_in)\n",
    "        else:\n",
    "            x_b = x_in\n",
    "        branches = [conv(x_b) for conv in self.conv_branches]\n",
    "        branches.append(self.maxpool_branch(x_in))\n",
    "        x_cat = torch.cat(branches, dim=1)\n",
    "        x_cat = self.batchnorm(x_cat)\n",
    "        if self.use_residual:\n",
    "            x_res = self.residual(x_in)\n",
    "            x_cat = x_cat + x_res\n",
    "        x_cat = self.activation(x_cat)\n",
    "        x_cat = self.dropout(x_cat)\n",
    "        return x_cat.transpose(1, 2)\n",
    "\n",
    "class InceptionTime(nn.Module):\n",
    "    def __init__(self, in_dim, num_blocks=3, out_channels=32, \n",
    "                 kernel_sizes=[10,20,40], bottleneck_channels=32, \n",
    "                 use_residual=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        blocks = []\n",
    "        channels = in_dim\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(InceptionModule(\n",
    "                in_channels=channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                use_residual=use_residual,\n",
    "                dropout=dropout\n",
    "            ))\n",
    "            channels = out_channels * (len(kernel_sizes) + 1)\n",
    "        self.network = nn.Sequential(*blocks)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features)\n",
    "        out = self.network(x)\n",
    "        # out: (batch, seq_len, channels)\n",
    "        out = out.transpose(1, 2)            # -> (batch, channels, seq_len)\n",
    "        pooled = self.global_pool(out).squeeze(2)\n",
    "        return self.classifier(pooled).squeeze(1)\n",
    "\n",
    "model = InceptionTime(\n",
    "    in_dim=len(feature_columns),\n",
    "    num_blocks=3,\n",
    "    out_channels=32,\n",
    "    kernel_sizes=[3,5,7],\n",
    "    bottleneck_channels=32,\n",
    "    use_residual=True,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# 7) Loss & optimizer (class-weighted)\n",
    "pos_w = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.tensor(pos_w, device=device, dtype=torch.float32)\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "\n",
    "# 8) Train with early stopping on Val AUC\n",
    "best_val_auc = 0.0\n",
    "patience, trials = 5, 0\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in dl_train:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / len(dl_train.dataset)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl_val:\n",
    "            xb = xb.to(device)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            preds.extend(probs)\n",
    "            trues.extend(yb.numpy())\n",
    "    val_auc = roc_auc_score(trues, preds)\n",
    "    scheduler.step(val_auc)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {avg_loss:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc, trials = val_auc, 0\n",
    "        torch.save(model.state_dict(), 'best_inception.pth')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# 9) Test set evaluation\n",
    "model.load_state_dict(torch.load('best_inception.pth'))\n",
    "model.eval()\n",
    "test_preds, test_trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in dl_test:\n",
    "        xb = xb.to(device)\n",
    "        test_preds.extend(torch.sigmoid(model(xb)).cpu().numpy())\n",
    "        test_trues.extend(yb.numpy())\n",
    "\n",
    "y_hat = (np.array(test_preds) > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(test_trues, y_hat, digits=4))\n",
    "print(\"Confusion Matrix (low values = better balance):\")\n",
    "print(confusion_matrix(test_trues, y_hat))\n",
    "\n",
    "roc = roc_auc_score(test_trues, test_preds)\n",
    "prec, rec, _ = precision_recall_curve(test_trues, test_preds)\n",
    "print(f\"\\nTest ROC AUC:  {roc:.4f}\")\n",
    "print(f\"Test PR AUC:   {auc(rec, prec):.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(test_trues, y_hat):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
